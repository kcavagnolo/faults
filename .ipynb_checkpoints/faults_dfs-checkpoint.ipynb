{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* problem statment does not suggest device independence: these could be networked or interdependent systems\n",
    "* classes are HEAVILY imbalanced; standard concerns on local optima\n",
    "* [during failure, attributes 2, 4, 7, and 8 report much higher values than normal. This is not true of 1, 3, 5, 6, and 9](#failhard)\n",
    "* [a1 and a6 are highly unique](#uniqueness)\n",
    "* [no duplicate data and no missing values](#missing)\n",
    "* [events appear to be ordered](#ordered)\n",
    "* [devices appear to come in types/classes](#deviceclasses) \n",
    "* [scale or one-hot encoide attributes](#attributevals)\n",
    "* [a2 & a4 ramp-up before failure, and looks like a7 spikes quickly](#correlation)\n",
    "* [a3 & a9 are weakly positively correlated](#correlation)\n",
    "* [a7 & a8 are perfectly positively correlated... because they are identical](#a7a8)\n",
    "* [a7 appears to be critical in predicting failure in a naive model](#naivemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.471750Z",
     "start_time": "2018-07-30T22:12:15.457564Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# science\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# system\n",
    "import random\n",
    "import hashlib\n",
    "import pickle\n",
    "import tempfile\n",
    "import timeit\n",
    "\n",
    "# visuals\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from IPython import display\n",
    "\n",
    "# options\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "sns_cmap = sns.diverging_palette(10, 220, sep=80, n=5)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# in case nb needs to be re-run\n",
    "try:\n",
    "    %load_ext autoreload\n",
    "except:\n",
    "    %reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.494717Z",
     "start_time": "2018-07-30T22:12:16.473734Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    '''\n",
    "    Load a generic CSV file into a pandas dataframe\n",
    "    '''\n",
    "    df = pd.read_csv(filename, escapechar='\\\\', encoding='utf-8')\n",
    "    df['uuid'] = df.apply(lambda x: hashlib.md5(str(x.values).encode('utf-8')).hexdigest(), axis=1)\n",
    "    assert sum(df.set_index('uuid').index.duplicated()) == 0, 'Error loading data: Hash collision; duplicate data'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.526285Z",
     "start_time": "2018-07-30T22:12:16.496731Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_nan_values(orig_df):\n",
    "    df = orig_df.copy()\n",
    "    df = df.isna().sum(axis=0).reset_index()\n",
    "    df.columns = ['column_name', 'na_count']\n",
    "    df['na_ratio'] = df['na_count'] / orig_df.shape[0]\n",
    "    df = df.loc[df['na_ratio'] > 0]\n",
    "    df = df.sort_values(by='na_ratio')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.546549Z",
     "start_time": "2018-07-30T22:12:16.528238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_null_values(orig_df):\n",
    "    df = orig_df.copy()\n",
    "    df = df.isnull().sum(axis=0).reset_index()\n",
    "    df.columns = ['column_name', 'null_count']\n",
    "    df['null_ratio'] = df['null_count'] / orig_df.shape[0]\n",
    "    df = df.loc[df['null_ratio'] > 0]\n",
    "    df = df.sort_values(by='null_ratio')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.565155Z",
     "start_time": "2018-07-30T22:12:16.548435Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def value_limits(df, field, upper_bound=99, lower_bound=1):\n",
    "    upper_limit = np.percentile(df[field].values, upper_bound)\n",
    "    lower_limit = np.percentile(df[field].values, lower_bound)\n",
    "    return upper_limit, lower_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.587100Z",
     "start_time": "2018-07-30T22:12:16.567020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_correlations(df, fields):\n",
    "\n",
    "    # keep a copy\n",
    "    df = df.copy()\n",
    "\n",
    "    # correlation coefficient of each column\n",
    "    for field in fields:\n",
    "        x_cols = [col for col in df.columns\n",
    "                  if col != field and (df[col].dtype=='float64' or df[col].dtype=='int64')]\n",
    "        labels = []\n",
    "        values = []\n",
    "        for col in x_cols:\n",
    "            labels.append(col)\n",
    "            values.append(\n",
    "                np.corrcoef(df[col].values, df[field].values)[0, 1])\n",
    "\n",
    "        # create dataframe for corr coeffs\n",
    "        corr_df = pd.DataFrame({'col_labels': labels, 'corr_values': values})\n",
    "        corr_df = corr_df.sort_values(by='corr_values')\n",
    "\n",
    "        ind = np.arange(len(labels))\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\n",
    "        ax.set_yticks(ind)\n",
    "        ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n",
    "        ax.set_xlabel('correlation coefficient')\n",
    "        ax.set_title('Correlations with {}'.format(field))\n",
    "        ax.set_xlim(-1, 1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:16.686897Z",
     "start_time": "2018-07-30T22:12:16.662966Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_values(df, fields):\n",
    "    \n",
    "    # keep a copy\n",
    "    df = df.copy()\n",
    "\n",
    "    # iterate over fields\n",
    "    for field in fields:\n",
    "\n",
    "        # get limits of data\n",
    "        ulimit, llimit = value_limits(df, field)\n",
    "        print('###################')\n",
    "        print('Working on field {}'.format(field))\n",
    "        print('Upper limit: {:.3f}'.format(ulimit))\n",
    "        print('Lower limit: {:.3f}'.format(llimit))\n",
    "\n",
    "        # plot the ordered values for field\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(range(df.shape[0]), np.sort(df[field].values))\n",
    "        plt.xlabel('index', fontsize=12)\n",
    "        plt.ylabel(field, fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "        # plot histo of values\n",
    "        sns.distplot(df[field].values, bins=50, kde=True, norm_hist=True)\n",
    "        plt.xlabel(field, fontsize=12)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:17.067165Z",
     "start_time": "2018-07-30T22:12:17.047591Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size):\n",
    "    x = Input(shape=(input_size,))\n",
    "    layer1 = Dense(25, activation='relu')(x)\n",
    "    layer2 = Dense(10)(layer1)\n",
    "    layer3 = Dense(35)(layer2)\n",
    "    layer4 = Dense(10)(layer3)\n",
    "    layer5 = Dense(25, activation=None)(layer4)\n",
    "    layer6 = Dense(output_size)(layer5)\n",
    "    model = Model(x, layer6)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_absolute_error',\n",
    "                  metrics=['mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:17.223085Z",
     "start_time": "2018-07-30T22:12:17.202055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_hdf5(df, hdf_file, table_name, format=None):\n",
    "    if format is not None:\n",
    "        df.to_hdf(\n",
    "            hdf_file,\n",
    "            table_name,\n",
    "            mode='w',\n",
    "            format=format,\n",
    "            data_columns=True,\n",
    "            complevel=9,\n",
    "            complib='blosc:lz4')\n",
    "    else:\n",
    "        df.to_hdf(\n",
    "            hdf_file,\n",
    "            table_name,\n",
    "            mode='w',\n",
    "            data_columns=True,\n",
    "            complevel=9,\n",
    "            complib='blosc:lz4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:17.349949Z",
     "start_time": "2018-07-30T22:12:17.329269Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cohen_kappa_scorer(preds, dmatrix):\n",
    "    y = dmatrix.get_label()\n",
    "    cohen_kappa_score(preds, y)\n",
    "    return 'kappa', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:12:18.283988Z",
     "start_time": "2018-07-30T22:12:18.260195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_xgb_model(X_train, y_train, params, kfolds=10, random_seed=7):\n",
    "\n",
    "    # set a timer\n",
    "    starttime = timeit.default_timer()\n",
    "\n",
    "    # setup\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # isolate train features\n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.astype(int).values\n",
    "    \n",
    "    # stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=kfolds, random_state=42, shuffle=False)\n",
    "    for i, (train_index, cv_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print('Working on k-fold {}...'.format(i))\n",
    "        \n",
    "        # split data\n",
    "        print('Splitting data')\n",
    "        X_ktrain, X_kcv = X_train[train_index], X_train[cv_index]\n",
    "        y_ktrain, y_kcv = y_train[train_index], y_train[cv_index]\n",
    "        \n",
    "        # Convert our data into XGBoost format\n",
    "        print('Converting data to xgb format')        \n",
    "        d_train = xgb.DMatrix(X_ktrain, label=y_ktrain, feature_names=feature_names)\n",
    "        d_cv    = xgb.DMatrix(X_kcv, label=y_kcv, feature_names=feature_names)\n",
    "        watchlist = [(d_train, 'train'), (d_cv, 'cv')]\n",
    "\n",
    "        # train the model\n",
    "        print('Modeling')\n",
    "        model = xgb.train(params, d_train, num_boost_round=3000,\n",
    "                          evals=watchlist, #feval=cohen_kappa_scorer,\n",
    "                          early_stopping_rounds=300, verbose_eval=200)\n",
    "        print('Done.')\n",
    "    \n",
    "    # testing\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sensitivity(y_test, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_test * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_test, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def specificity(y_test, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_test) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_test, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:12:18.671262Z",
     "start_time": "2018-07-27T17:12:07.733140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "faults_df = load_data(\"device_failure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:12:18.841068Z",
     "start_time": "2018-07-27T17:12:18.739586Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see a sample\n",
    "faults_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:12:19.006441Z",
     "start_time": "2018-07-27T17:12:18.933099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rename columns for less typing\n",
    "faults_df.columns = faults_df.columns.str.replace(\"attribute\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:12:19.283691Z",
     "start_time": "2018-07-27T17:12:19.102695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# odd date format, did load reformat?\n",
    "!head device_failure.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:12:19.697545Z",
     "start_time": "2018-07-27T17:12:19.397758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# basic summary for any obvious weirdness\n",
    "faults_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This data set appears to have attributes that are \"codes\" and thus categorical. The min/max and stddev for each is large, so there's a spread. One-hot encoding will be... tough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# QA/QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:36:58.701804Z",
     "start_time": "2018-07-27T15:36:58.388774Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# per problem statement, one device per day?\n",
    "check = faults_df.date.astype(str) + faults_df.device.astype(str)\n",
    "assert len(check) == len(check.drop_duplicates()), 'More than one device per day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:36:58.953372Z",
     "start_time": "2018-07-27T15:36:58.912537Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inspect data types\n",
    "dtype_df = faults_df.dtypes.reset_index()\n",
    "dtype_df.columns = [\"Count\", \"Column Type\"]\n",
    "dtype_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-25T17:45:33.435265Z",
     "start_time": "2018-07-25T17:45:33.394287Z"
    },
    "hidden": true
   },
   "source": [
    "A1 is very unique: device identifier? <a id=\"uniqueness\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:36:59.480246Z",
     "start_time": "2018-07-27T15:36:59.177855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# inspect uniqueness\n",
    "uniqueness_df = pd.DataFrame(faults_df.nunique()).reset_index()\n",
    "uniqueness_df.columns = [\"column\", \"num_unique\"]\n",
    "uniqueness_df['perc_unique'] = uniqueness_df.num_unique.apply(lambda x: x/len(faults_df))\n",
    "uniqueness_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Nothing missing <a id=\"missing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:36:59.791516Z",
     "start_time": "2018-07-27T15:36:59.697685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nulls\n",
    "calculate_null_values(faults_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:37:00.122747Z",
     "start_time": "2018-07-27T15:37:00.032084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nan's (just to be sure)\n",
    "calculate_nan_values(faults_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:37:00.412938Z",
     "start_time": "2018-07-27T15:37:00.372160Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# look at columns\n",
    "for c in sorted(faults_df.columns):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Attribute values vary wildly <a id=\"attributevals\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:25:38.665072Z",
     "start_time": "2018-07-26T21:25:16.744169Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize column values\n",
    "fields = [c for c in faults_df.columns if faults_df[c].dtype == 'int64']\n",
    "visualize_values(faults_df, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some attributes have orders of magnitude higher values than others within same category. Must have units of measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:25:38.808316Z",
     "start_time": "2018-07-26T21:25:38.766268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = faults_df.filter(regex='^a').columns.tolist()\n",
    "temp = faults_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:02:03.315995Z",
     "start_time": "2018-07-26T21:00:31.872078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(temp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:02:09.355491Z",
     "start_time": "2018-07-26T21:02:08.964172Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"failure\", data=faults_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yikes, this is an extreme example of imbalances classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T18:01:20.503098Z",
     "start_time": "2018-07-27T18:01:20.454472Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "100.*faults_df.groupby('failure').failure.count()/len(faults_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"Extreme\" is putting it lightly: 99.91% of the time, the data indicates non-failure. **Model #1**: Always predict 'non-failure', you'll be right most of the time :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:24:12.946362Z",
     "start_time": "2018-07-27T17:24:12.682817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = faults_df.filter(regex='^a|failure')\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(temp) \n",
    "temp.loc[:,:] = scaled_values\n",
    "temp_melt = pd.melt(temp, \"failure\", var_name=\"measurement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T15:38:26.179387Z",
     "start_time": "2018-07-27T15:38:26.139239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.swarmplot(x=\"measurement\", y=\"value\", hue=\"failure\", data=temp_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:18:37.738322Z",
     "start_time": "2018-07-27T17:18:22.051151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"measurement\", y=\"value\", hue=\"failure\", kind=\"bar\", data=temp_melt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some attributes have values ONLY when a failure occurs: a2, a4, a7, a8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:53:50.526259Z",
     "start_time": "2018-07-27T17:53:50.447668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subtemp = temp.filter(regex='a2|a3|a9|a4|a7|a8|failure')\n",
    "for c in subtemp.columns:\n",
    "    subtemp.loc[subtemp[c] > 0, c] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:53:51.248833Z",
     "start_time": "2018-07-27T17:53:51.093315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subtemp_melt = pd.melt(subtemp, \"failure\", var_name=\"measurement\")\n",
    "grouped = subtemp_melt.groupby(['measurement', 'failure']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:53:51.911459Z",
     "start_time": "2018-07-27T17:53:51.867689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Far more values present when not failing, so there must be a slant toward large values when there is a failure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:54:07.596354Z",
     "start_time": "2018-07-27T17:53:56.372249Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"measurement\", y=\"value\", hue=\"failure\", kind=\"bar\", data=subtemp_melt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id=\"failhard\"></a>\n",
    "Looks to be true. When there is a failure, attributes 2, 4, 7, and 8 report much higher values than normal. This is not true of 1, 3, 5, 6, and 9. They should be correlated with failure then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T17:58:55.337146Z",
     "start_time": "2018-07-27T17:58:54.874679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "corr = temp.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That is confidence building: a2, a4, a7, and a8 are weakly correlated with failure. Dig on A7==A8 in EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:25:39.014490Z",
     "start_time": "2018-07-26T21:25:38.970157Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what are the dates representing?\n",
    "dates = faults_df.date\n",
    "dates.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dates look ordered <a id=\"ordered\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:25:45.326751Z",
     "start_time": "2018-07-26T21:25:39.143400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# is it sorted?\n",
    "all(dates[i] <= dates[i+1] for i in range(len(dates)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:25:45.469376Z",
     "start_time": "2018-07-26T21:25:45.429807Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# so these are ordered, arbitrarily assigned dates representations\n",
    "pd.Series(dates.unique()).diff().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-23T21:13:21.452630Z",
     "start_time": "2018-07-23T21:12:10.360788Z"
    },
    "hidden": true
   },
   "source": [
    " Can't create datetime augmentations; Day of week, time of month, et al. may be important, cannot hardcode unknown information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:25:45.635087Z",
     "start_time": "2018-07-26T21:25:45.579971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add order val in case its useful\n",
    "faults_df.insert(0, 'order', range(0, len(faults_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As long as a date transformation is applied uniformly such that relative information is preserved, creating a datetime object with an arbitrary reference is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:03.766777Z",
     "start_time": "2018-07-26T21:25:45.746279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "faults_df['datetime'] = faults_df['date'].apply(lambda x: pd.to_datetime(x, unit='d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:28.173978Z",
     "start_time": "2018-07-26T21:26:28.130314Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# isolate devices\n",
    "devices = faults_df.device\n",
    "devices.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:28.826857Z",
     "start_time": "2018-07-26T21:26:28.788747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# num of devices\n",
    "len(devices.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:29.323435Z",
     "start_time": "2018-07-26T21:26:28.995755Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# how many chars in each field of device name?\n",
    "maxchars = max(devices.apply(lambda x: len(x)))\n",
    "for nchar in range(0, maxchars):\n",
    "    print('Char position {} :: num unique {}'.format(nchar,\n",
    "                                                     len(devices.apply(lambda x: x[nchar]).unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:30.025884Z",
     "start_time": "2018-07-26T21:26:29.519451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first four chars don't vary much\n",
    "# are there flavors of naming conventions, e.g. device classes?\n",
    "maxchars = max(devices.apply(lambda x: len(x)))\n",
    "for nchars in range(0, maxchars):\n",
    "    print('num chars {} :: num unique {}'.format(nchars,\n",
    "                                              len(devices.apply(lambda x: x[:nchars]).unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I'm getting suspicious that there is time ordering which could be informative: a device failure tree. So want to keep that ordering going forward -- no groupby's! Cross device failures may be important. <a id=\"deviceclasses\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:30.639410Z",
     "start_time": "2018-07-26T21:26:30.572761Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save the class1\n",
    "faults_df['device_class1'] = faults_df.device.apply(lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:31.217200Z",
     "start_time": "2018-07-26T21:26:31.179941Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "faults_df.device_class1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:31.875010Z",
     "start_time": "2018-07-26T21:26:31.548801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove class1 and look for more\n",
    "sub_devices = devices.apply(lambda x: x[3:])\n",
    "maxchars = max(sub_devices.apply(lambda x: len(x)))\n",
    "for nchars in range(1, maxchars):\n",
    "    print('num chars {} :: num unique {}'.format(nchars,\n",
    "                                              len(sub_devices.apply(lambda x: x[:nchars]).unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:32.205219Z",
     "start_time": "2018-07-26T21:26:32.150213Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maybe 4th char is a status? small enough to encode\n",
    "faults_df['device_class2'] = faults_df.device.apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:32.979888Z",
     "start_time": "2018-07-26T21:26:32.939129Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "faults_df.device_class2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:33.388168Z",
     "start_time": "2018-07-26T21:26:33.120055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove class1 and class2 and look for more\n",
    "sub_devices = devices.apply(lambda x: x[4:])\n",
    "maxchars = max(sub_devices.apply(lambda x: len(x)))\n",
    "for nchars in range(1, maxchars):\n",
    "    print('num chars {} :: num unique {}'.format(nchars,\n",
    "                                              len(sub_devices.apply(lambda x: x[:nchars]).unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:33.759573Z",
     "start_time": "2018-07-26T21:26:33.693715Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lump remainder together\n",
    "faults_df['device_class3'] = faults_df.device.apply(lambda x: x[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:34.329279Z",
     "start_time": "2018-07-26T21:26:34.268852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "faults_df.device_class3.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:34.749866Z",
     "start_time": "2018-07-26T21:26:34.649837Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check that original device == device clases\n",
    "sum(faults_df.device != faults_df.device_class1 + faults_df.device_class2 + faults_df.device_class3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:42.480473Z",
     "start_time": "2018-07-26T21:26:42.165997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "faults_df.to_pickle('faults_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-27T19:01:41.010Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        # pick random device that had a failure and look at attribute trends in time\n",
    "        temp = faults_df[(faults_df.failure == 1)]\n",
    "        rand_device = temp.sample(1).device.values[0]\n",
    "        temp = faults_df[(faults_df.device == rand_device)]\n",
    "        temp = temp.filter(regex='^a|date')\n",
    "        temp['a1cs'] = temp.a1.cumsum()\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        scaled_values = scaler.fit_transform(temp)\n",
    "        temp.loc[:,:] = scaled_values\n",
    "        temp.plot(x='date', style='.-')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "a1 looks like a daily value, a6 is an aggregate. a2 and a4 clearly stand-out as ramping up before failure, but is that common regardless? A few attributes are correlated it appears. <a id=\"correlation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:48.917630Z",
     "start_time": "2018-07-26T21:26:45.167830Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look for correlations in numerical values\n",
    "fields = [c for c in faults_df.columns if faults_df[c].dtype == 'int64']\n",
    "visualize_correlations(faults_df, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A7 and A8 identical? <a id=\"a7a8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:49.390925Z",
     "start_time": "2018-07-26T21:26:49.068671Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# are attribute 7 and 8 identical?\n",
    "a7_str = ''.join(faults_df.a7.astype(str).values)\n",
    "a7_hash = hashlib.md5(a7_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "a8_str = ''.join(faults_df.a8.astype(str).values)\n",
    "a8_hash = hashlib.md5(a8_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "a7_hash == a8_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is telemetry data, so maybe A7 and A8 are the same measurement, different units, e.g. imperial and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:51.243434Z",
     "start_time": "2018-07-26T21:26:51.193127Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 7 was my fav num as a kid (Boomer Esiason wore it for the Bengals)\n",
    "# drop attr8\n",
    "faults_df.drop('a8', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:26:53.614008Z",
     "start_time": "2018-07-26T21:26:51.865234Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corrmat = faults_df.corr(method='spearman')\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.clustermap(corrmat, vmax=1., square=True, cmap=sns_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Weak similarities, but nothing so glaring we should break attributes apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is a gnarly imbalanced class problem. Traditional options:\n",
    "* undersample: this is time series data, so arbitrarily throwing out samples removes valuable information.\n",
    "* oversample: I dislike the idea of duplicate data since it assumes events are reproducible.\n",
    "* synthesis: I intensely dislike creating data unless there are strong underlying laws/principles, e.g. in physics, which constrain the fake data.\n",
    "* stratified resampling: this is a modeling technique we'll have to use.\n",
    "\n",
    "Normally, we would put in a great deal of effort and time with SME's to build features. For example, cumulative sums of aggregated values over time should be important, and we can engineer such a set of features with:\n",
    "```python\n",
    "for col in ['a'+str(i) for i in range(1,10)]:\n",
    "    newcol = col+'_cumsum'\n",
    "    try:\n",
    "        faults_df.drop([newcol], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        faults_df[newcol] = faults_df.groupby(['device'])[col].cumsum()\n",
    "    except:\n",
    "        continue\n",
    "```\n",
    "\n",
    "However, what about all the other stats terms that may have embeded info (count, max, min, mean, stddev, etc.). This is laborious to construct all those features. Alternatively, we could use a carefully tuned NN architecture to find those features via the model. This is also laborious. Another option: create an ensemble of models that extract predictive information from features it creates.\n",
    "\n",
    "As a start, let's instead try deep feature synthesis and drive the problem to the left (data) rather than to the right (model). This can be accomplished with time series using [tensor deep feature synthesis][tdfs].\n",
    "\n",
    "[tdfs]: https://docs.featuretools.com/automated_feature_engineering/handling_time.html#creating-a-3-dimensional-feature-tensor-using-multiple-cutoff-times-from-make-temporal-cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cut-off Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:17.420306Z",
     "start_time": "2018-07-26T21:27:17.031353Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of unique devices\n",
    "unique_devices = faults_df.filter(regex='device').drop_duplicates()\n",
    "instance_ids = sorted(unique_devices.device.tolist())\n",
    "\n",
    "# capture start dates per device -- ORDER MATTERS\n",
    "device_start_dates = faults_df.sort_values(by=['device']).groupby('device')['datetime'].min().tolist()\n",
    "\n",
    "# capture end dates per device -- ORDER MATTERS\n",
    "device_end_dates = faults_df.sort_values(by=['device']).groupby('device')['datetime'].max().tolist()\n",
    "\n",
    "# create reference data frame\n",
    "cutoffs = pd.DataFrame(\n",
    "    {\n",
    "        'instance_ids': instance_ids,\n",
    "        'start': device_start_dates,\n",
    "        'cutoffs': device_end_dates,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:21.540867Z",
     "start_time": "2018-07-26T21:27:17.942811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gut check on 100 random entries that reassembly of dates and devices match\n",
    "for n in range(0, 100):\n",
    "    \n",
    "    # pick random device\n",
    "    rand_device = faults_df.sample(1).device.values[0]\n",
    "    \n",
    "    # get original min/max dates in np datetime\n",
    "    min_test = faults_df[(faults_df.device == rand_device)]['datetime'].min().to_datetime64()\n",
    "    max_test = faults_df[(faults_df.device == rand_device)]['datetime'].max().to_datetime64()\n",
    "    \n",
    "    # get cutoff values\n",
    "    rand_min = cutoffs[(cutoffs.instance_ids == rand_device)].start.values[0]\n",
    "    rand_max = cutoffs[(cutoffs.instance_ids == rand_device)].cutoffs.values[0]\n",
    "    \n",
    "    # compare\n",
    "    assert rand_min == min_test, 'Date mismatch'\n",
    "    assert rand_max == max_test, 'Date mismatch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:24.699279Z",
     "start_time": "2018-07-26T21:27:21.721633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for each device, begin at start and increment by 1-day until the end\n",
    "temporal_cutoffs = ft.make_temporal_cutoffs(\n",
    "    instance_ids=cutoffs['instance_ids'],\n",
    "    start=cutoffs['start'],\n",
    "    cutoffs=cutoffs['cutoffs'],\n",
    "    window_size='1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:24.926527Z",
     "start_time": "2018-07-26T21:27:24.882376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save\n",
    "temporal_cutoffs.to_pickle('temporal_cutoffs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Deep Feature Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:28.163423Z",
     "start_time": "2018-07-26T21:27:28.136352Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create new entityset\n",
    "es = ft.EntitySet(id='device_faults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:29.929455Z",
     "start_time": "2018-07-26T21:27:29.882418Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create entity for devices\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id='unique_devices',\n",
    "    dataframe=unique_devices,\n",
    "    index='device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:30.888794Z",
     "start_time": "2018-07-26T21:27:30.853351Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "es['unique_devices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:32.435474Z",
     "start_time": "2018-07-26T21:27:31.753733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create entity for faults\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id='faults',\n",
    "    dataframe=faults_df,\n",
    "    index='uuid',\n",
    "    time_index='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:33.025464Z",
     "start_time": "2018-07-26T21:27:32.986001Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "es['faults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:33.990532Z",
     "start_time": "2018-07-26T21:27:33.883297Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Relationship between clients and previous loans\n",
    "relation_device_faults = ft.Relationship(es['unique_devices']['device'],\n",
    "                                         es['faults']['device'])\n",
    "\n",
    "# Add the relationship to the entity set\n",
    "es = es.add_relationship(relation_device_faults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:35.220890Z",
     "start_time": "2018-07-26T21:27:35.193702Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:42.060527Z",
     "start_time": "2018-07-26T21:27:36.123191Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save results\n",
    "es.to_parquet('device_faults_entity_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:43.133157Z",
     "start_time": "2018-07-26T21:27:42.357220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reload data\n",
    "es = ft.read_parquet('device_faults_entity_set')\n",
    "temporal_cutoffs = pd.read_pickle('temporal_cutoffs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The below step can be parallelized using Dask (`n_jobs=-1`) since it's an embarassingly parallel problem due to independence:\n",
    "\n",
    "$device_i \\perp cutoff_j \\perp feature matrix_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:49.457640Z",
     "start_time": "2018-07-26T21:27:49.392683Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reminder of the dfs primitives\n",
    "ft.primitives.list_primitives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T21:27:53.290695Z",
     "start_time": "2018-07-26T21:27:53.260583Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# primitive selections\n",
    "agg_primitives=[\"mean\", \"median\", \"mode\", \"max\", \"min\", \"std\", \"skew\",\n",
    "                \"all\", \"sum\", \"count\", \"num_unique\", \"trend\",\n",
    "                \"last\", \"time_since_last\", \"avg_time_between\"]\n",
    "\n",
    "trans_primitives=[\"cum_count\", \"cum_sum\", \"cum_mean\", \"cum_max\",\n",
    "                  \"time_since_previous\",\n",
    "                  \"mod\", \"add\", \"subtract\", \"diff\", \"divide\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ignore_variables = ['uuid',\n",
    "                    'date',\n",
    "                    'device_class1',\n",
    "                    'device_class2',\n",
    "                    'device_class3',\n",
    "                    'datetime',\n",
    "                    'failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-26T22:41:49.425772Z",
     "start_time": "2018-07-26T21:27:57.475345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the feature tensor\n",
    "#tempfile = tempfile.NamedTemporaryFile(prefix=\"dfs_\")\n",
    "feature_tensor, feature_defs = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='unique_devices',\n",
    "    cutoff_time=temporal_cutoffs,\n",
    "    cutoff_time_in_index=True,\n",
    "    agg_primitives=agg_primitives,\n",
    "    trans_primitives=trans_primitives,\n",
    "    ignore_variables=ignore_variables,\n",
    "    max_depth=1,\n",
    "    max_features=-1,\n",
    "    n_jobs=-1,\n",
    "    #save_progress='./{}'.format(tempfile.name),  # broken in curr ver of FT\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:38:49.683879Z",
     "start_time": "2018-07-27T14:38:44.204659Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save intermediate output\n",
    "hdf_file = 'raw_dfs_feature_tensor.h5'\n",
    "table_name = 'feature_tensor'\n",
    "save_hdf5(feature_tensor, hdf_file, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## DFS QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:07:59.559390Z",
     "start_time": "2018-07-27T14:07:58.760819Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# clean-up data and don't disturb original\n",
    "dfs_qa = feature_tensor.copy().reset_index()\n",
    "dfs_qa.rename(columns={'time':'datetime'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:08:06.966624Z",
     "start_time": "2018-07-27T14:08:06.935985Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data in == data out\n",
    "orig_len = len(dfs_qa)\n",
    "new_len = len(feature_tensor)\n",
    "assert orig_len == new_len, 'There is new or missing data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The '1d' time windows in DFS automatically create steps in time where there may not be data, e.g.:\n",
    "* original: [1 2 3 4 8 9]\n",
    "* dfs: [1 2 3 4 **_5 6 7_** 8 9]\n",
    "\n",
    "There are added values. Simply joing back against original dataframe (need to do anyways) fixes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:09:10.277292Z",
     "start_time": "2018-07-27T14:09:08.823841Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# per problem statement, one device per day?\n",
    "check = dfs_qa.datetime.astype(str) + dfs_qa.device.astype(str)\n",
    "assert len(check) == len(check.drop_duplicates()), 'More than one device per day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:19:45.568333Z",
     "start_time": "2018-07-27T14:19:41.463322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rid ourselves of the junk datetime features due to arbitrary date assignment\n",
    "dropcols = dfs_qa.filter(regex='DAY|MONTH|WEEKDAY|YEAR').columns.tolist()\n",
    "nunique = dfs_qa.apply(pd.Series.nunique)\n",
    "dropcols.extend(nunique[nunique == 1].index.tolist())\n",
    "list(set(dropcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:20:04.629609Z",
     "start_time": "2018-07-27T14:20:04.514256Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop them\n",
    "dfs_qa.drop(dropcols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:22:05.459800Z",
     "start_time": "2018-07-27T14:22:04.935926Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# merge dfs with original faults\n",
    "faults_dfs_df = pd.merge(faults_df, dfs_qa, on=['device','datetime'], suffixes=('', '_y'))\n",
    "assert len(faults_dfs_df) == len(faults_df), 'Something went wrong in merge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:22:11.342434Z",
     "start_time": "2018-07-27T14:22:10.979587Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# some duplicate columns in join\n",
    "dropcols = faults_dfs_df.filter(regex='_y')\n",
    "faults_dfs_df.drop(dropcols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:40:39.251393Z",
     "start_time": "2018-07-27T14:40:18.365999Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 10 min runtime for above on laptop, save output just in case\n",
    "hdf_file = 'qa_dfs_feature_tensor.h5'\n",
    "table_name = 'feature_tensor'\n",
    "save_hdf5(faults_dfs_df, hdf_file, table_name, format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:41:13.007594Z",
     "start_time": "2018-07-27T14:41:09.712134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reload data\n",
    "hdf_file = 'qa_dfs_feature_tensor.h5'\n",
    "table_name = 'feature_tensor'\n",
    "Xy = pd.read_hdf(hdf_file, table_name)\n",
    "feature_names = Xy.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:41:27.303601Z",
     "start_time": "2018-07-27T14:41:22.949087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# some dfs are uniform\n",
    "nunique = Xy.apply(pd.Series.nunique)\n",
    "dropcols = nunique[nunique == 1].index.tolist()\n",
    "sorted(dropcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:41:34.665423Z",
     "start_time": "2018-07-27T14:41:34.635730Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove uniform cols\n",
    "if len(dropcols) > 0:\n",
    "    Xy.drop(dropcols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:41:35.480052Z",
     "start_time": "2018-07-27T14:41:35.408542Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:41:51.341354Z",
     "start_time": "2018-07-27T14:41:51.297385Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:56:21.136482Z",
     "start_time": "2018-07-27T14:56:21.103107Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# columns to encode\n",
    "encode_cols = [\n",
    "    'device_class1',\n",
    "    'device_class2',\n",
    "    'device_class3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:56:27.760210Z",
     "start_time": "2018-07-27T14:56:24.075824Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# one-hot encode the categoricals\n",
    "Xy = pd.concat([Xy, pd.get_dummies(Xy[encode_cols])], axis=1)\n",
    "Xy.drop(encode_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:56:55.518452Z",
     "start_time": "2018-07-27T14:56:55.082312Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# junk features\n",
    "dropcols = Xy.filter(regex='MODE\\(faults.device_class')\n",
    "Xy.drop(dropcols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T14:59:25.168907Z",
     "start_time": "2018-07-27T14:59:18.748552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save intermediate output\n",
    "hdf_file = 'model_dfs_feature_tensor.h5'\n",
    "table_name = 'feature_tensor'\n",
    "save_hdf5(Xy, hdf_file, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:22:14.022319Z",
     "start_time": "2018-07-30T23:22:05.297373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# reload data from prior run as a quick start\n",
    "faults_df = load_data(\"device_failure.csv\")\n",
    "hdf_file = 'model_dfs_feature_tensor.h5'\n",
    "table_name = 'feature_tensor'\n",
    "Xy = pd.read_hdf(hdf_file, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:22:15.026361Z",
     "start_time": "2018-07-30T23:22:14.024569Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ID cols to use in modeling\n",
    "ignore = ['date', 'device', 'datetime']\n",
    "ignore.extend(Xy.filter(regex='failure').columns)  # anything with failure info\n",
    "predicting = 'failure'\n",
    "predictors = [c for c in Xy.columns if (c != predicting and c not in ignore)]\n",
    "\n",
    "# isolate y from X\n",
    "y = Xy[predicting]\n",
    "X = Xy[predictors]\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=7)\n",
    "\n",
    "# remove uuid and track as index\n",
    "idx_train = X_train.uuid.values\n",
    "idx_test = X_test.uuid.values\n",
    "X_train.drop('uuid', axis=1, inplace=True)\n",
    "X_test.drop('uuid', axis=1, inplace=True)\n",
    "feature_names = X_train.columns.values\n",
    "assert set(feature_names) == set(X_test.columns.values), 'Train/Test feature name mismatch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:22:40.248475Z",
     "start_time": "2018-07-30T23:22:37.155504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make the train test vers for xgb\n",
    "dtrain = xgb.DMatrix(X_train, y_train, feature_names=feature_names, nthread=-1)\n",
    "dtest = xgb.DMatrix(X_test, y_test, feature_names=feature_names, nthread=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T18:02:40.869131Z",
     "start_time": "2018-07-30T18:02:34.224766Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define a naive model\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'max_bin': 1024,\n",
    "    'max_leaves': 255,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'num_boost_round': 500,\n",
    "    'objective': 'binary:logistic',\n",
    "    'silent': 1,\n",
    "    'seed': 0,\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'grow_policy': 'depthwise',\n",
    "    'n_gpus': 1\n",
    "}\n",
    "model = xgb.train(xgb_params, dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id=\"#naivemodel\"></a>\n",
    "Is a7 really that important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T18:02:47.935937Z",
     "start_time": "2018-07-30T18:02:46.958616Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(10, 18))\n",
    "xgb.plot_importance(model, height=0.6, ax=ax, max_num_features=10)\n",
    "fig.savefig('feature_importance.png', bbox_inches='tight', pad_inches=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T18:23:14.118347Z",
     "start_time": "2018-07-30T18:23:13.069345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T18:20:10.648052Z",
     "start_time": "2018-07-30T18:20:10.112857Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T18:27:21.015063Z",
     "start_time": "2018-07-27T18:27:20.622733Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# let's see if this makes sense\n",
    "temp = faults_df.filter(regex='^a|failure')\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_values = scaler.fit_transform(temp)\n",
    "temp.loc[:,:] = scaled_values\n",
    "temp_melt = pd.melt(temp, \"failure\", var_name=\"measurement\")\n",
    "temp_melt.groupby(['measurement', 'failure']).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "stddev for a4 and a7 are an order of magnitude larger when there's a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T18:51:07.038285Z",
     "start_time": "2018-07-27T18:51:06.992979Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:47:41.825109Z",
     "start_time": "2018-07-30T23:22:43.453789Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# control for class imbalance\n",
    "ratio = float(np.sum(y == 0)) / np.sum(y == 1)\n",
    "\n",
    "# model params\n",
    "params = {\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 100,\n",
    "    'subsample': 0.9,  # random sample %\n",
    "    'colsample_bytree': 0.4,  # random col %\n",
    "    'eta': 0.01,  # learning rate\n",
    "    'reg_alpha': 0.5,  # L1 reg\n",
    "    'lambda': 0.95,  # L2 reg\n",
    "    'gamma': 0.1,  # loss split\n",
    "    'seed': 6,\n",
    "    'n_estimators': 1000,\n",
    "    'scale_pos_weight': ratio,  # VERY IMPORTANT FOR THIS PARTICULAR PROBLEM\n",
    "    'objective': 'gpu:binary:logistic',\n",
    "    #'objective': 'multi:softprob',\n",
    "    #'num_class': 2,\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'grow_policy': 'depthwise'\n",
    "}\n",
    "\n",
    "# train the model\n",
    "model = train_xgb_model(X_train, y_train, params, kfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:56:10.353406Z",
     "start_time": "2018-07-30T23:56:09.301314Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# feature importance plot\n",
    "fig, ax = plt.subplots(figsize=(10, 18))\n",
    "xgb.plot_importance(model, height=0.6, ax=ax, max_num_features=10)\n",
    "fig.savefig('feature_importance.png', bbox_inches='tight', pad_inches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:56:29.461400Z",
     "start_time": "2018-07-30T23:56:29.382070Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = model.predict(dtest, ntree_limit=model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:56:47.033810Z",
     "start_time": "2018-07-30T23:56:45.941170Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binary predictions\n",
    "target_names = ['no fail', 'fail']\n",
    "for prob_thresh in np.arange(0.4950, 0.4951, 0.00001):\n",
    "    y_pred_binary = [1 if x > prob_thresh else 0 for x in y_pred ]\n",
    "    print(prob_thresh)\n",
    "    print(metrics.classification_report(y_test, y_pred_binary, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T00:02:27.716896Z",
     "start_time": "2018-07-31T00:02:27.625696Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# select threshold\n",
    "y_pred_binary = [1 if x >= 0.49502 else 0 for x in y_pred ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T00:02:28.517195Z",
     "start_time": "2018-07-31T00:02:27.944124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualize prec-recall\n",
    "average_precision = metrics.average_precision_score(y_test, y_pred)\n",
    "precision, recall, _ = metrics.precision_recall_curve(y_test, y_pred)\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Binary Precision-Recall: Avg Precision={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T00:02:29.673965Z",
     "start_time": "2018-07-31T00:02:29.157703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cm\n",
    "ax = sns.heatmap(metrics.confusion_matrix(y_test, y_pred_binary), annot=True, annot_kws={\"size\": 16});\n",
    "ax.set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T00:04:22.480705Z",
     "start_time": "2018-07-31T00:04:22.276408Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# roc\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:13:21.730883Z",
     "start_time": "2018-07-30T22:13:21.710702Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:13:22.276530Z",
     "start_time": "2018-07-30T22:13:22.219025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h2o.init()\n",
    "h2o.cluster().show_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:14:43.135010Z",
     "start_time": "2018-07-30T22:13:28.222808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_df_h2o = h2o.H2OFrame(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:14:43.281668Z",
     "start_time": "2018-07-30T22:14:43.137304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train, test, valid = train_df_h2o.split_frame(ratios=[0.7, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:14:46.172024Z",
     "start_time": "2018-07-30T22:14:43.288140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Identify predictors and response\n",
    "x = train[predictors].columns\n",
    "y = predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-30T22:15:30.479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Run AutoML\n",
    "aml = H2OAutoML(max_runtime_secs=3600, balance_classes=True)\n",
    "aml.train(x=x,\n",
    "          y=y,\n",
    "          training_frame=train,\n",
    "          validation_frame=valid,\n",
    "          leaderboard_frame=test)#,\n",
    "          #class_sampling_factors=\n",
    "          #max_after_balance_size=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:16:57.239281Z",
     "start_time": "2018-07-30T23:16:57.216845Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:16:57.985071Z",
     "start_time": "2018-07-30T23:16:57.926688Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T23:17:09.306940Z",
     "start_time": "2018-07-30T23:17:08.973701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The leader model is stored here\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "AutoML \"likes\" SKEW(a4) like xgboost did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T00:07:12.339368Z",
     "start_time": "2018-07-31T00:07:11.545406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred = aml.leader.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## FFNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just kidding. There is too little information on the data set to justify building a network of arbitrary architecture. For example, if devices are connected or if device_class<n> is one part of a single entity, then this impacts the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class_weight={\n",
    "    1: n_non_cancer_samples / n_cancer_samples * t\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(0.001),\n",
    "    metrics=[sensitivity, specificity]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-27T21:23:55.742383Z",
     "start_time": "2018-07-27T21:23:55.703553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "686px",
    "left": "638px",
    "right": "20px",
    "top": "110px",
    "width": "731px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
